{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# İçerik\n",
    "* [Gerekli Kütüphaneler](#Gerekli-Kütüphaneler)\n",
    "* [Çıkarım Bazlı Özetleme](#Çıkarım-Bazlı-Özetleme)\n",
    "* [Örnek Kullanım](#Örnek-Kullanım)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerekli Kütüphaneler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import heapq\n",
    "from gensim.summarization import keywords\n",
    "from nltk import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Çıkarım Bazlı Özetleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extraction_based_sum():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Modelimizi kokbulma.json dosyasından okuyoruz.\n",
    "        self.jstr = json.loads(open('kokbulma.json').read())\n",
    "        self.model = model_from_json(self.jstr)\n",
    "        # Sonrasında model.hdf5 dosyasından önceden eğitilmiş 1.2 milyon kelimelik ağırlıklarımızı alıyoruz.\n",
    "        self.model.load_weights('model.hdf5')\n",
    "        # trmodel.dms[2] Türkçe Word2Vec modeli için kullandığımız hazır bir model.\n",
    "        self.word_tr = KeyedVectors.load_word2vec_format('trmodel.dms', binary=True)\n",
    "        # datafile.pkl dosyasının içerisinde Türkçe harfler, kelime uzunluğu gibi özellikler tutuluyor.\n",
    "        fp = open('datafile.pkl','rb')\n",
    "        data = pickle.load(fp)\n",
    "        fp.close()\n",
    "        self.chars = data['chars']\n",
    "        self.charlen = data['charlen']\n",
    "        self.maxlen = data['maxlen']\n",
    "        \n",
    "    def encode(self,word,maxlen=22,is_pad_pre=False):\n",
    "        # Bu methodda, kelimelerimizin uzunluklarını kontrol ediyoruz,\n",
    "        # ve kelimelerimizi matris formuna dönüştürüyoruz.\n",
    "        wlen = len(str(word))\n",
    "        if wlen > maxlen:\n",
    "            word = word[:maxlen]\n",
    "\n",
    "        word = str(word).lower()\n",
    "        pad = maxlen - len(word)\n",
    "        if is_pad_pre :\n",
    "            word = pad*' '+word   \n",
    "        else:\n",
    "            word = word + pad*' '\n",
    "        mat = []\n",
    "        for w in word:\n",
    "            vec = np.zeros((self.charlen))\n",
    "            if w in self.chars:\n",
    "                ix = self.chars.index(w)\n",
    "                vec[ix] = 1\n",
    "            mat.append(vec)\n",
    "        return np.array(mat)\n",
    "\n",
    "    def decode(self,mat):\n",
    "        # Encode methodunda oluşturulan matrisi bu methodda tekrar kelimeye dönüştürüyoruz.\n",
    "        word = \"\"\n",
    "        for i in range(mat.shape[0]):\n",
    "            word += self.chars[np.argmax(mat[i,:])]\n",
    "        return word.strip()\n",
    "    \n",
    "    def kokBul(self,word):\n",
    "        # Bu methodda ise encoder ve decoder methodları kullanılarak elimizdeki kelimenin modelimize göre\n",
    "        # kök sonucunu buluyoruz.\n",
    "        X = []\n",
    "        w = self.encode(word)\n",
    "        X.append(w)\n",
    "        X = np.array(X)\n",
    "        yp = self.model.predict(X)\n",
    "        return self.decode(yp[0])\n",
    "    \n",
    "    def cleanText(self,text):\n",
    "        \n",
    "        # Bu methodda, elimizdeki metnin temizliğini, 1.2 milyon kelimeyle üretilmiş, kök bulma konusunda\n",
    "        # %99.94 başarı oranına sahip 'Ka|Ve Stemmer' modelimizle köklerine ayırıp Türkçe'deki \n",
    "        # durak kelimelerinden(stopwords) arındırarak TextRank algoritmasının daha iyi sonuçlar vermesini\n",
    "        # sağlıyoruz. Kullandığımız model deeplearningtürkiye'nin 'Kelime Kök Ayırıcı' modeli üzerine \n",
    "        # ve TsCorpus'un sağladığı kök analizi sonuçlarına göre kendimiz oluşturduk.[1][4]\n",
    "        \n",
    "        text_file = open(\"turkce-stop-words.txt\", \"r\")\n",
    "        lines = text_file.readlines()\n",
    "        self.stop_words = []\n",
    "        for line in lines:\n",
    "            self.stop_words.append(line[:-1])\n",
    "        self.stop_words.append('bir')\n",
    "        self.stop_words.append('bin')\n",
    "        text = re.sub(r'[\\s]',' ',text)\n",
    "        sentences = sent_tokenize(text)\n",
    "        self.clean_sentences = []\n",
    "        for sentence in sentences:\n",
    "            temp_list = []\n",
    "            for word in sentence.split():\n",
    "                if (word.lower() not in self.stop_words) and (len(word) >= 2):\n",
    "                    temp_list.append(self.kokBul(word))\n",
    "            self.clean_sentences.append(' '.join(temp_list))\n",
    "            \n",
    "        # Bu kısımda ise Hasan Kemik tarafından önceden oluşturulmuş 'Çıkarım Tabanlı Metin Özetleme'\n",
    "        # kodu[3] üzerine 'Ka|Ve Stemmer' modülü entegre edilerek geliştirilmiştir.\n",
    "        # Word2Vec modeline göre benzerlik matrisi oluşturduktan sonra, networkx kütüphanesi kullanılarak,\n",
    "        # cümle skorlarına karar veriyoruz.\n",
    "\n",
    "        sentence_vectors = []\n",
    "        for sentence in self.clean_sentences:\n",
    "            for word in sentence.split():\n",
    "                try:\n",
    "                    v = word_tr[word.lower()]\n",
    "                except:\n",
    "                    v = np.zeros(400)\n",
    "                sentence_vectors.append(v)\n",
    "        sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences)):\n",
    "                if i != j:\n",
    "                    sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,400), sentence_vectors[j].reshape(1,400))[0,0]\n",
    "        nx_graph = nx.from_numpy_array(sim_mat)\n",
    "        scores = nx.pagerank(nx_graph)\n",
    "        ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "        return ranked_sentences\n",
    "    \n",
    "    def get_sentences(self,text,sum_length):\n",
    "        # Bu methodda ise, temizlediğimiz ve skorladığımız metnimizden 'n' tane cümleyi özet olarak\n",
    "        # sisteme geri dönüyoruz.\n",
    "        ranked_sentences = self.cleanText(text)\n",
    "        summary = []\n",
    "        for i in range(sum_length):\n",
    "            summary.append(ranked_sentences[i][1])\n",
    "        return \" \".join(summary)\n",
    "        \n",
    "    \n",
    "    def get_keywords(self,text,ratio):\n",
    "        # Bu methodda ise, gensim kütüphanesinin anahtar kelime çıkarım mekanizması kullanılarak,\n",
    "        # metindeki en önemki stop word olmayan kelimelerin bulunmasını hedefledik\n",
    "        x = self.cleanText(text)\n",
    "        text_keywords = keywords(text,ratio=ratio).split(\"\\n\")\n",
    "        valid_keywords = []\n",
    "        for keyword in text_keywords:\n",
    "            if keyword not in self.stop_words:\n",
    "                valid_keywords.append(keyword)\n",
    "        return valid_keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Örnek Kullanım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_sum = extraction_based_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Transition-One adlı girişim, donanım iyileştirme teknolojisiyle eski dizel araçları elektrikli araca dönüştürüyor.\n",
    "\n",
    "Fransız girişimi Transition-One, eski dizel araçlara 8 bin 500 Euro karşılığında elektrik motoru, batarya ve bağlantılı bir gösterge paneli ekleyen donanım iyileştirme teknolojisi geliştirdi.\n",
    "\n",
    "Transition-One kurucusu Aymeric Libeau “Yeni bir elektrikli arabaya 20 bin Euro veremeyecek durumdaki insanlara ulaşmayı hedefliyorum.” diyor. 2009 model bir Renault Twingo’yu 180 kilometre menzilli bir elektrikli araca dönüştürdüğü ilk prototipini gösteren Libeau “Avrupa’da en çok satılan modelleri elektrikli arabalara dönüştürüyoruz.” dedi.\n",
    "\n",
    "Dönüşüm bir günden az sürüyor.\n",
    "\n",
    "Libeau, bu yılın sonuna kadar Fransız ve Avrupalı düzenleyicilerden onay almayı umuyor. Ayrıca talep durumunu test etmek için Eylül ayında ön sipariş almaya başlayacak. Otomobil üreticileri, Avrupa’daki katı karbon salınımı düzenlemelerine uyabilmek için hızla elektrikli araba üretmeye çalışıyor. Eski dizel arabaları yasaklayan şehirlerin sayısı her geçen gün artıyor. Önümüzdeki on yıl içinde de çok daha fazla Avrupa şehri fosil yakıtlı arabalara erişimi kesecek.\n",
    "\n",
    "Libeau’nun yöntemiyle dizel aracı elektrikliye dönüştürme işlemi bir günden az sürüyor.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Önümüzdeki on yıl içinde de çok daha fazla Avrupa şehri fosil yakıtlı arabalara erişimi kesecek. Transition-One kurucusu Aymeric Libeau “Yeni bir elektrikli arabaya 20 bin Euro veremeyecek durumdaki insanlara ulaşmayı hedefliyorum.” diyor. Otomobil üreticileri, Avrupa’daki katı karbon salınımı düzenlemelerine uyabilmek için hızla elektrikli araba üretmeye çalışıyor. Libeau’nun yöntemiyle dizel aracı elektrikliye dönüştürme işlemi bir günden az sürüyor. Libeau, bu yılın sonuna kadar Fransız ve Avrupalı düzenleyicilerden onay almayı umuyor.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_sum.get_sentences(text,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['libeau',\n",
       " 'elektrikli araca',\n",
       " 'avrupa',\n",
       " 'eski dizel',\n",
       " 'icin',\n",
       " 'arabalara',\n",
       " 'fransız',\n",
       " 'gun',\n",
       " 'artıyor',\n",
       " 'euro',\n",
       " 'iyilestirme',\n",
       " 'girisim donanım',\n",
       " 'almaya',\n",
       " 'motoru',\n",
       " 'duzenleyicilerden onay almayı',\n",
       " 'sonuna',\n",
       " 'sehirlerin',\n",
       " 'baslayacak',\n",
       " 'elektrik',\n",
       " 'umuyor']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_sum.get_keywords(text,0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

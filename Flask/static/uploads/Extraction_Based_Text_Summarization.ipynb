{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "* [Necessary Libraries](#Necessary-Libraries)\n",
    "* [Extraction Based Sum as Class](#Extraction-Based-Sum-as-Class)\n",
    "* [Usage Example](#Usage-Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import heapq\n",
    "from gensim.summarization import keywords\n",
    "from nltk import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction Based Sum as Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extraction_based_sum():\n",
    "    def __init__(self):\n",
    "        self.jstr = json.loads(open('kokbulma.json').read())\n",
    "        self.model = model_from_json(self.jstr)\n",
    "        self.model.load_weights('model.hdf5')\n",
    "        self.word_tr = KeyedVectors.load_word2vec_format('trmodel.dms', binary=True)\n",
    "        fp = open('datafile.pkl','rb')\n",
    "        data = pickle.load(fp)\n",
    "        fp.close()\n",
    "        self.chars = data['chars']\n",
    "        self.charlen = data['charlen']\n",
    "        self.maxlen = data['maxlen']\n",
    "        \n",
    "    def encode(self,word,maxlen=22,is_pad_pre=False):\n",
    "        wlen = len(str(word))\n",
    "        if wlen > maxlen:\n",
    "            word = word[:maxlen]\n",
    "\n",
    "        word = str(word).lower()\n",
    "        pad = maxlen - len(word)\n",
    "        if is_pad_pre :\n",
    "            word = pad*' '+word   \n",
    "        else:\n",
    "            word = word + pad*' '\n",
    "        mat = []\n",
    "        for w in word:\n",
    "            vec = np.zeros((self.charlen))\n",
    "            if w in self.chars:\n",
    "                ix = self.chars.index(w)\n",
    "                vec[ix] = 1\n",
    "            mat.append(vec)\n",
    "        return np.array(mat)\n",
    "\n",
    "    def decode(self,mat):\n",
    "        word = \"\"\n",
    "        for i in range(mat.shape[0]):\n",
    "            word += self.chars[np.argmax(mat[i,:])]\n",
    "        return word.strip()\n",
    "    \n",
    "    def kokBul(self,word):\n",
    "        X = []\n",
    "\n",
    "        w = self.encode(word)\n",
    "        X.append(w)\n",
    "\n",
    "        X = np.array(X)\n",
    "\n",
    "        yp = self.model.predict(X)\n",
    "        return self.decode(yp[0])\n",
    "    \n",
    "    def cleanText(self,text):\n",
    "        \n",
    "        text_file = open(\"turkce-stop-words.txt\", \"r\")\n",
    "        lines = text_file.readlines()\n",
    "        self.stop_words = []\n",
    "        for line in lines:\n",
    "            self.stop_words.append(line[:-1])\n",
    "        self.stop_words.append('bir')\n",
    "        self.stop_words.append('bin')\n",
    "        text = re.sub(r'[\\s]',' ',text)\n",
    "        sentences = sent_tokenize(text)\n",
    "        self.clean_sentences = []\n",
    "        for sentence in sentences:\n",
    "            temp_list = []\n",
    "            for word in sentence.split():\n",
    "                if (word.lower() not in self.stop_words) and (len(word) >= 2):\n",
    "                    temp_list.append(self.kokBul(word))\n",
    "            self.clean_sentences.append(' '.join(temp_list))\n",
    "        sentence_vectors = []\n",
    "        for sentence in self.clean_sentences:\n",
    "            for word in sentence.split():\n",
    "                try:\n",
    "                    v = word_tr[word.lower()]\n",
    "                except:\n",
    "                    v = np.zeros(400)\n",
    "                sentence_vectors.append(v)\n",
    "        sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences)):\n",
    "                if i != j:\n",
    "                    sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,400), sentence_vectors[j].reshape(1,400))[0,0]\n",
    "        nx_graph = nx.from_numpy_array(sim_mat)\n",
    "        scores = nx.pagerank(nx_graph)\n",
    "        ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "        return ranked_sentences\n",
    "    \n",
    "    def get_sentences(self,text,sum_length):\n",
    "        ranked_sentences = self.cleanText(text)\n",
    "        summary = []\n",
    "        for i in range(sum_length):\n",
    "            summary.append(ranked_sentences[i][1])\n",
    "        return \" \".join(summary)\n",
    "        \n",
    "    \n",
    "    def get_keywords(self,text,ratio):\n",
    "        text_keywords = keywords(text,ratio=ratio).split(\"\\n\")\n",
    "        valid_keywords = []\n",
    "        for keyword in text_keywords:\n",
    "            if keyword not in self.stop_words:\n",
    "                valid_keywords.append(keyword)\n",
    "        return valid_keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_sum = extraction_based_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Transition-One adlı girişim, donanım iyileştirme teknolojisiyle eski dizel araçları elektrikli araca dönüştürüyor.\n",
    "\n",
    "Fransız girişimi Transition-One, eski dizel araçlara 8 bin 500 Euro karşılığında elektrik motoru, batarya ve bağlantılı bir gösterge paneli ekleyen donanım iyileştirme teknolojisi geliştirdi.\n",
    "\n",
    "Transition-One kurucusu Aymeric Libeau “Yeni bir elektrikli arabaya 20 bin Euro veremeyecek durumdaki insanlara ulaşmayı hedefliyorum.” diyor. 2009 model bir Renault Twingo’yu 180 kilometre menzilli bir elektrikli araca dönüştürdüğü ilk prototipini gösteren Libeau “Avrupa’da en çok satılan modelleri elektrikli arabalara dönüştürüyoruz.” dedi.\n",
    "\n",
    "Dönüşüm bir günden az sürüyor.\n",
    "\n",
    "Libeau, bu yılın sonuna kadar Fransız ve Avrupalı düzenleyicilerden onay almayı umuyor. Ayrıca talep durumunu test etmek için Eylül ayında ön sipariş almaya başlayacak. Otomobil üreticileri, Avrupa’daki katı karbon salınımı düzenlemelerine uyabilmek için hızla elektrikli araba üretmeye çalışıyor. Eski dizel arabaları yasaklayan şehirlerin sayısı her geçen gün artıyor. Önümüzdeki on yıl içinde de çok daha fazla Avrupa şehri fosil yakıtlı arabalara erişimi kesecek.\n",
    "\n",
    "Libeau’nun yöntemiyle dizel aracı elektrikliye dönüştürme işlemi bir günden az sürüyor.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Önümüzdeki on yıl içinde de çok daha fazla Avrupa şehri fosil yakıtlı arabalara erişimi kesecek. Transition-One kurucusu Aymeric Libeau “Yeni bir elektrikli arabaya 20 bin Euro veremeyecek durumdaki insanlara ulaşmayı hedefliyorum.” diyor. Otomobil üreticileri, Avrupa’daki katı karbon salınımı düzenlemelerine uyabilmek için hızla elektrikli araba üretmeye çalışıyor. Libeau’nun yöntemiyle dizel aracı elektrikliye dönüştürme işlemi bir günden az sürüyor. Libeau, bu yılın sonuna kadar Fransız ve Avrupalı düzenleyicilerden onay almayı umuyor.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_sum.get_sentences(text,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ex_sum_5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e55254e10539>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mex_sum_5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ex_sum_5' is not defined"
     ]
    }
   ],
   "source": [
    "ex_sum_5.get_keywords(text,0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
